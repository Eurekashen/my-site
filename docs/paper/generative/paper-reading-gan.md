---
title: GAN
date: 2023-10-02
author: Shuaike Shen
tags: ['Paper reading','GAN']
math: true
categories: 
- Paper reading
- Generative Model
---

理论部分，关于概率论的推导部分没有细看，主要在讲证明目标函数的选择是正确的以及优化目标函数。注意理解一下琴声-香浓散度、KL散度

GAN模型的训练过程非常的有意思，使用的是没有标号的数据——也就是说属于无监督学习。但是GAN的目标函数是使用的有label的，其label就是数据是真实的还是模型生成出来的。这推动了之后的弱监督学习和半监督学习的发展。

- GAN模型是一个framework
  - 生成模型G、辨别模型D，生成模型的任务就是让辨别模型犯错，让辨别模型无法分辨生成出来的图片和真实的图片
  - 在D、G都是一个MLP的情况下可以通过一个反向传播来更新权重而不需要使用马尔可夫链，计算过程相对的简单。
  - 在这个框架下生成模型和辨别模型都是MLP，输入是高斯分布的随机噪声。这种情况下，只需要通过反向传播而不需要计算马尔可夫链，D、G是同时进行训练的。
- 深度学习和GAN：
  - 深度神经网络属于深度学习，深度学习内涵是使用一个模型、函数去拟合数据的概率分布。
  - 在辨别模型上有非常大的进展
  - 但是深度学习在生成模型上的进展不足，这是因为深度学习要最大化去似然函数，这带来了很多计算上的困难。
- GAN的形象描述：像造假者和警察，两个人会不断的对抗；造假造的越来越真，警察辨别假的的能力也越来越强；到最后的结果就是造假的人造出来的东西和真的没有区别。警察分不出来真的和假的。对应GAN中的生成模型和辨别模型。
- 两种途径：
  - 计算出来真正的分布是什么样的。（计算量大，更加难以计算）
  - 用一个函数来去拟合，不管真正分布是什么样的。在这里使用的是一个MLP（理论上MLP可以拟合任何函数）
- 目标函数
  -  $$min_Gmax_DV(D, G) = E_{x∼pdata(x)}[log D(x)] + E_{z∼pz(z)}[log(1 − D(G(z)))]$$

  -  这是博弈论中经典的双人min-max游戏。
- 训练过程：
  - <img src="/sreenshortcut/Screenshot 2023-08-19 at 14.08.46.png">

  -  黑色虚线代表真实数据，绿线代表生成器数据、蓝线代表辨别器数据。x代表真实数据空间，z表示噪声的空间。
- 具体算法如下：

<img src="/sreenshortcut/Screenshot 2023-08-19 at 14.11.06.png">

一次迭代先更新辨别器再更新生成器。在这里k是一个超参数，让D、G的更新在进度上是差不多的，两者才有对抗的空间，模型才能够训练起来。 重要的是判断模型的收敛，GAN的收敛是非常不稳定：一边不动一边抖动，两方都在抖动这算不算收敛难以界定。

- 优点和缺点：
  - 难以收敛，训练过程非常的不平滑。必须平衡D、G，才能够生成出来比较好的结果。
  - 端到端学习，从图片到图片，没有中间的过程。
  - 不需要大量的标注数据

补充知识：

- KL散度：

  - > KL（Kullback-Leibler）散度，也称为相对熵（Relative Entropy），是一种用于衡量两个概率分布之间差异的度量方式。它用来量化从一个概率分布（真实分布）到另一个概率分布（近似分布或模型分布）的信息丢失程度，或者说是从一个分布中获得真实分布所需的额外信息量。

!!! quote
     给定两个离散概率分布 $$P(x)$$ 和 $$Q(x)$$，KL散度定义为：
    
     $$D_{KL}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$$
    
     对于连续分布，KL散度定义为：
    
     $$D_{KL}(P \parallel Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx$$
    
     值得注意的是，KL散度并不是对称的，即 $$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$。这意味着在实际应用中，选择哪个分布作为“真实分布”和“近似分布”会影响KL散度的值。
    
     KL散度有以下几个重要性质：
    
     1. 非负性： $$D_{KL}(P \parallel Q) \geq 0$$，当且仅当 $$P(x) = Q(x)$$ 时取等号。
     2. 非对称性： $$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$。
     3. 不满足三角不等式：一般情况下， $$D_{KL}(P \parallel R) \nleq D_{KL}(P \parallel Q) + D_{KL}(Q \parallel R)$$。
    
    在机器学习中，KL散度常用于测量模型分布与真实数据分布之间的差异，例如在生成模型中用于衡量生成的样本分布与真实数据分布之间的差异。最小化KL散度或者等价地最大化KL散度的负值（KL散度的对偶形式），在许多情况下可以用来优化模型参数，使模型分布逼近真实数据分布。